======== DISCUSSION ========

The lowest perplexities (~13) were achieved using the MLE estimate 
(no delta smoothing). In other words, the predictive power of 
the language model degrades when even minimal weighting is given towards 
unseen bigrams. This suggests that a vast majority of the test corpus 
can be predicted using only the grams from the training corpus.

English had an MLE perplexity of 13.75 whereas 
French had an MLE perplexity of 13.06. 
This rough equivalence was maintained regardless of delta,
suggesting that the language model is equally well suited 
for training on either language.

Overall, perplexity increased as the smoothing factor increased (see results below).
There was a small decrease between delta=0.0001 and 0.001 (43,39)->(39,36),
and an increase between delta=0.001 and 0.01 (39,36)->(41,41).
This suggests that a better smoothing factor exists 
(probably with perplexity around 36-40) somewhere in the 
interval of 0.0001 and 0.01, but this is unlikely to be better 
than the MLE perplexity (~13).
======== RESULTS    ========
Note: Delta=0 means no smoothing (MLE estimate)
Language, Delta, Perplexity
E, 0, 13.7509
F, 0, 13.0622
E, 0.0001, 43.0674
F, 0.0001, 39.0133
E, 0.001, 39.4475
F, 0.001, 36.7718
E, 0.01, 41.8078
F, 0.01, 41.1484
E, 0.1, 60.0325
F, 0.1, 63.8216
E, 1, 131.3166
F, 1, 153.0768
